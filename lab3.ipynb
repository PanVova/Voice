{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYEHSaCEIvM-",
    "outputId": "6b8af52b-afa9-496e-fc74-681babfb7568",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:00.373045Z",
     "start_time": "2025-06-03T12:33:55.844663Z"
    }
   },
   "source": [
    "!pip install torch torchaudio pandas tqdm\n",
    "!pip install -U gdown\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\samurai\\miniconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\samurai\\miniconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\samurai\\miniconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samurai\\miniconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: gdown in c:\\users\\samurai\\miniconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from gdown) (3.17.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\samurai\\miniconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\samurai\\miniconda3\\lib\\site-packages (0.22.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\samurai\\miniconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samurai\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:01.273363Z",
     "start_time": "2025-06-03T12:34:00.529296Z"
    }
   },
   "cell_type": "code",
   "source": "!pip show torch",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.7.0+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: C:\\Users\\Samurai\\miniconda3\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
      "Required-by: asteroid-filterbanks, julius, lightning, nnAudio, pyannote.audio, pytorch-lightning, pytorch-metric-learning, speechbrain, torch-audiomentations, torch_pitch_shift, torchaudio, torchmetrics, torchvision\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_GIoYzphIezA",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:02.888367Z",
     "start_time": "2025-06-03T12:34:01.276368Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:02.920251Z",
     "start_time": "2025-06-03T12:34:02.895964Z"
    }
   },
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.version())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n",
      "90100\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "lbEb8868TAyO",
    "outputId": "af934ab6-bb0e-4092-dbc2-085df21310e1",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:02.931072Z",
     "start_time": "2025-06-03T12:34:02.923256Z"
    }
   },
   "source": [
    "# Download data set\n",
    "file_path = 'C:/Users/Samurai/'"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1DvWYKsSI0Dp",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:02.942276Z",
     "start_time": "2025-06-03T12:34:02.938402Z"
    }
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "#\n",
    "# def extract_archives(file_path):\n",
    "#     archives = {\n",
    "#         'voxconverse-master.zip': 'voxconverse-master/',\n",
    "#         'voxconverse_dev_wav.zip': 'dev_wav/',\n",
    "#         'voxconverse_test_wav.zip': 'test_wav/'\n",
    "#     }\n",
    "#\n",
    "#     for archive_name, extract_folder in archives.items():\n",
    "#         zip_path = os.path.join(file_path, archive_name)\n",
    "#         extract_path = os.path.join(file_path, extract_folder)\n",
    "#         with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#             zip_ref.extractall(extract_path)\n",
    "#\n",
    "# extract_archives(file_path)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sHQdDLkrSo7e",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:02.952540Z",
     "start_time": "2025-06-03T12:34:02.949237Z"
    }
   },
   "source": [
    "def read_rttm(rttm_path):\n",
    "    cols = ['TYPE', 'FILE', 'CHANNEL', 'START', 'DURATION', 'NA1', 'NA2', 'SPEAKER', 'NA3', 'NA4']\n",
    "    with open(rttm_path, 'r') as f:\n",
    "        lines = [line.strip().split() for line in f if line.startswith('SPEAKER')]\n",
    "    df = pd.DataFrame(lines, columns=cols)\n",
    "    df['START'] = df['START'].astype(float)\n",
    "    df['DURATION'] = df['DURATION'].astype(float)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9JafXAySqry",
    "outputId": "1f871c67-41f0-4df6-a7a2-f883d65699c4",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:06.254295Z",
     "start_time": "2025-06-03T12:34:02.960842Z"
    }
   },
   "source": [
    "def extract_segments(rttm_dir, wav_dir):\n",
    "    rttm_files = sorted(glob.glob(os.path.join(rttm_dir, '*.rttm')))\n",
    "    segments = []\n",
    "\n",
    "    for rttm_path in tqdm(rttm_files):\n",
    "        df = read_rttm(rttm_path)\n",
    "        audio_id = os.path.splitext(os.path.basename(rttm_path))[0]\n",
    "        audio_path = os.path.join(wav_dir, audio_id + '.wav')\n",
    "        if not os.path.exists(audio_path):\n",
    "            continue\n",
    "        for _, row in df.iterrows():\n",
    "            segments.append({\n",
    "                'audio_path': audio_path,\n",
    "                'start': row['START'],\n",
    "                'end': row['START'] + row['DURATION'],\n",
    "                'speaker': row['SPEAKER']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(segments)\n",
    "\n",
    "\n",
    "segments_df = extract_segments(\n",
    "    file_path + 'voxconverse-master/voxconverse-master/dev',\n",
    "    file_path + 'dev_wav/audio'\n",
    ")\n",
    "\n",
    "segments_df_test = extract_segments(\n",
    "    file_path + 'voxconverse-master/voxconverse-master/test',\n",
    "    file_path + 'test_wav/voxconverse_test_wav'\n",
    ")\n",
    "\n",
    "print(segments_df.head())\n",
    "print(segments_df_test.head())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:01<00:00, 147.84it/s]\n",
      "100%|██████████| 232/232 [00:01<00:00, 129.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 audio_path   start     end speaker\n",
      "0  C:/Users/Samurai/dev_wav/audio\\abjxc.wav    0.40    7.04   spk00\n",
      "1  C:/Users/Samurai/dev_wav/audio\\abjxc.wav    8.68   64.64   spk00\n",
      "2  C:/Users/Samurai/dev_wav/audio\\afjiv.wav   41.12   80.48   spk00\n",
      "3  C:/Users/Samurai/dev_wav/audio\\afjiv.wav  140.64  141.64   spk01\n",
      "4  C:/Users/Samurai/dev_wav/audio\\afjiv.wav  142.20  144.32   spk01\n",
      "                                          audio_path  start    end speaker\n",
      "0  C:/Users/Samurai/test_wav/voxconverse_test_wav...   2.50   6.16   spk00\n",
      "1  C:/Users/Samurai/test_wav/voxconverse_test_wav...   6.20  11.54   spk01\n",
      "2  C:/Users/Samurai/test_wav/voxconverse_test_wav...  11.68  14.62   spk00\n",
      "3  C:/Users/Samurai/test_wav/voxconverse_test_wav...  14.96  18.40   spk01\n",
      "4  C:/Users/Samurai/test_wav/voxconverse_test_wav...  19.09  25.44   spk01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OaMYvzL8SqoZ",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:06.267849Z",
     "start_time": "2025-06-03T12:34:06.263357Z"
    }
   },
   "source": [
    "class VoxConverseSegmentDataset(Dataset):\n",
    "    def __init__(self, segments, segment_duration=3.0, sample_rate=16000):\n",
    "        self.segments = segments\n",
    "        self.segment_duration = segment_duration\n",
    "        self.sample_rate = sample_rate\n",
    "        self.speaker_to_idx = {s: i for i, s in enumerate(sorted(segments['speaker'].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.segments.iloc[idx]\n",
    "        speaker_idx = self.speaker_to_idx[row['speaker']]\n",
    "        audio_segment = self._load_segment(row['audio_path'], row['start'], row['end'])\n",
    "        return audio_segment, speaker_idx\n",
    "\n",
    "    def _load_segment(self, audio_path, start, end):\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
    "\n",
    "        start_sample = int(start * self.sample_rate)\n",
    "        end_sample = int(end * self.sample_rate)\n",
    "        audio_segment = waveform[:, start_sample:end_sample]\n",
    "\n",
    "        target_len = int(self.segment_duration * self.sample_rate)\n",
    "        segment_len = audio_segment.shape[1]\n",
    "\n",
    "        if segment_len < target_len:\n",
    "            padding = target_len - segment_len\n",
    "            audio_segment = torch.nn.functional.pad(audio_segment, (0, padding))\n",
    "        else:\n",
    "            audio_segment = audio_segment[:, :target_len]\n",
    "\n",
    "        return audio_segment"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:06.303896Z",
     "start_time": "2025-06-03T12:34:06.300896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    audios, labels = zip(*batch)\n",
    "    audios = torch.stack(audios)\n",
    "    labels = torch.stack(labels)\n",
    "    return audios, labels"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXEWkXznSqlQ",
    "outputId": "11cedec1-d42b-46db-df5a-720aa8e86430",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:06.556128Z",
     "start_time": "2025-06-03T12:34:06.331206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = VoxConverseSegmentDataset(segments_df)\n",
    "loader = DataLoader(dataset, batch_size=4, pin_memory=True, shuffle=True)\n",
    "batch = next(iter(loader))"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p_tsS8L-Sqds",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:06.601767Z",
     "start_time": "2025-06-03T12:34:06.595259Z"
    }
   },
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "dataset_train = VoxConverseSegmentDataset(segments_df)\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, collate_fn = collate_fn, pin_memory=True, shuffle=True)\n",
    "\n",
    "dataset_test = VoxConverseSegmentDataset(segments_df_test)\n",
    "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, pin_memory=True, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zg_XXvkUqGn",
    "outputId": "9527c46e-b2a0-494d-fc50-ad9cd23b136b",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:06.640903Z",
     "start_time": "2025-06-03T12:34:06.636904Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleSpeakerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_speakers, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_speakers)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.classifier(x)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:06.767576Z",
     "start_time": "2025-06-03T12:34:06.666476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_speakers = len(dataset_train.speaker_to_idx)\n",
    "input_size = 16000 * 3\n",
    "model = SimpleSpeakerClassifier(input_size=input_size, num_speakers=num_speakers)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPuNKROSUtC5",
    "outputId": "9a6f67ba-364b-4c01-842b-36a792f2ff23",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:08.755611Z",
     "start_time": "2025-06-03T12:34:06.780610Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "print(f\"Dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (audio_batch, labels) in enumerate(tqdm(train_loader)):\n",
    "        audio_batch = audio_batch.to(device).squeeze(1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(audio_batch)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"[Epoch {epoch} | Batch {batch_idx}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch}] Average Loss: {avg_loss:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8268\n",
      "Number of batches: 1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1034 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     13\u001B[39m model.train()\n\u001B[32m     14\u001B[39m running_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (audio_batch, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tqdm(train_loader)):\n\u001B[32m     17\u001B[39m     audio_batch = audio_batch.to(device).squeeze(\u001B[32m1\u001B[39m)\n\u001B[32m     18\u001B[39m     labels = labels.to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[32m   1182\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[32m   1183\u001B[39m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[32m   1184\u001B[39m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28mself\u001B[39m._next_data()\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28mself\u001B[39m._dataset_fetcher.fetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.collate_fn(data)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 4\u001B[39m, in \u001B[36mcollate_fn\u001B[39m\u001B[34m(batch)\u001B[39m\n\u001B[32m      2\u001B[39m audios, labels = \u001B[38;5;28mzip\u001B[39m(*batch)\n\u001B[32m      3\u001B[39m audios = torch.stack(audios)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m labels = torch.stack(labels)\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m audios, labels\n",
      "\u001B[31mTypeError\u001B[39m: expected Tensor as element 0 in argument 0, but got int"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GmT9jUsAzNjj",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:08.764907700Z",
     "start_time": "2025-06-02T16:08:49.917400Z"
    }
   },
   "source": [
    "torch.save(model.state_dict(), 'speaker_model.pth')"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciU4KKQUUt0n",
    "outputId": "17780680-cc9c-4a22-f0a5-8204018a06c9",
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:08.764907700Z",
     "start_time": "2025-06-02T16:08:50.076491Z"
    }
   },
   "source": [
    "model.eval()\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for audio_batch, labels in tqdm(test_loader):\n",
    "        audio_batch = audio_batch.to(device).squeeze(1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(audio_batch)\n",
    "\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        correct_predictions += (predictions == labels.argmax(dim=1)).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2435/2435 [00:15<00:00, 155.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:08.764907700Z",
     "start_time": "2025-06-02T16:09:05.772693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "speaker_embeddings = []\n",
    "total_batches = len(test_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (audio_batch, labels) in enumerate(test_loader):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{total_batches}...\", end=\"\\r\")\n",
    "\n",
    "        audio_batch = audio_batch.to(device).squeeze(1)\n",
    "        emb = model(audio_batch, return_embedding=True)\n",
    "        speaker_embeddings.append(emb.cpu())\n",
    "\n",
    "speaker_embeddings = torch.cat(speaker_embeddings, dim=0)\n",
    "print(f\"Extracted {speaker_embeddings.shape[0]} embeddings\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 19478 embeddings...\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:08.774059700Z",
     "start_time": "2025-06-02T16:09:21.109862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "e1 = speaker_embeddings[0]\n",
    "e2 = speaker_embeddings[1]\n",
    "\n",
    "similarity = F.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)).item()\n",
    "\n",
    "print(f\"Cosine similarity: {similarity:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.8387\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
